{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import rnn,rnn_cell\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist=input_data.read_data_sets(\"/tmp/data/\",one_hot=True)\n",
    "\n",
    "def weight_variable(shape, name):\n",
    "    return tf.Variable(tf.truncated_normal(shape = shape, stddev = 0.1), name)\n",
    "\n",
    "def bias_variable(shape, name):\n",
    "    return tf.Variable(tf.zeros(shape = shape), name)\n",
    "\n",
    "def plot_n_reconstruct(origin_img, reconstruct_img, n = 10):\n",
    "\n",
    "    plt.figure(figsize=(2 * 10, 4))\n",
    "\n",
    "    for i in range(n):\n",
    "        # display original\n",
    "        ax = plt.subplot(2, n, i + 1)\n",
    "        plt.imshow(origin_img[i].reshape(28, 28))\n",
    "        plt.gray()\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "\n",
    "        # display reconstruction\n",
    "        ax = plt.subplot(2, n, i + 1 + n)\n",
    "        plt.imshow(reconstruct_img[i].reshape(28, 28))\n",
    "        plt.gray()\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_input=28\n",
    "n_steps=28\n",
    "n_hidden=128\n",
    "n_classes=10\n",
    "\n",
    "x=tf.placeholder(\"float\",[None,n_steps,n_input])\n",
    "y=tf.placeholder(\"float\",[None,n_classes])\n",
    "\n",
    "weights={\n",
    "    \"w_fc\":weight_variable([n_hidden,n_classes],\"w_fc\")\n",
    "}\n",
    "biases={\n",
    "    \"b_fc\":bias_variable([n_classes],\"b_fv\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_transpose shape: (28, ?, 28)\n",
      "x_reshape shape: (?, 28)\n",
      "type of x_split: <type 'list'>\n",
      "length of x_split: 28\n",
      "shape of x_split[0]: (?, 28)\n"
     ]
    }
   ],
   "source": [
    "x_transpose = tf.transpose(x, [1, 0, 2])\n",
    "print(\"x_transpose shape: %s\" % x_transpose.get_shape())\n",
    "\n",
    "x_reshape = tf.reshape(x_transpose, [-1, n_input])\n",
    "print(\"x_reshape shape: %s\" % x_reshape.get_shape())\n",
    "\n",
    "#order change in tf.split\n",
    "x_split = tf.split(x_reshape, n_steps, 0)\n",
    "print(\"type of x_split: %s\" % type(x_split))\n",
    "print(\"length of x_split: %d\" % len(x_split))\n",
    "print(\"shape of x_split[0]: %s\" % x_split[0].get_shape())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_rnn_cell=tf.contrib.rnn.BasicRNNCell(n_hidden)\n",
    "h,states=tf.contrib.rnn.static_rnn(basic_rnn_cell,x_split,dtype=tf.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fully connected layer\n",
    "\n",
    "h_fc=tf.matmul(h[-1],weights['w_fc']+biases['b_fc'])\n",
    "y_=h_fc\n",
    "\n",
    "#cost function\n",
    "cost=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=h_fc,labels=y))\n",
    "optimizer=tf.train.AdamOptimizer(0.01).minimize(cost)\n",
    "\n",
    "#accuracy function\n",
    "correct_prediction=tf.equal(tf.argmax(y_,1),tf.argmax(y,1))\n",
    "accuracy=tf.reduce_mean(tf.cast(correct_prediction,tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, loss 2.32661, accuracy 0.100, mean of rnn weight 0.00007, mean of rnn out -0.01312\n",
      "step 100, loss 2.38740, accuracy 0.110, mean of rnn weight 0.00041, mean of rnn out -0.00749\n",
      "step 200, loss 1.97791, accuracy 0.220, mean of rnn weight 0.00027, mean of rnn out -0.01693\n",
      "step 300, loss 2.53584, accuracy 0.150, mean of rnn weight 0.00156, mean of rnn out 0.03125\n",
      "step 400, loss 2.31662, accuracy 0.140, mean of rnn weight 0.00271, mean of rnn out 0.04397\n",
      "step 500, loss 2.33348, accuracy 0.070, mean of rnn weight 0.00271, mean of rnn out 0.04551\n",
      "step 600, loss 2.31514, accuracy 0.130, mean of rnn weight 0.00272, mean of rnn out 0.03230\n",
      "step 700, loss 2.37768, accuracy 0.110, mean of rnn weight 0.00273, mean of rnn out 0.03187\n",
      "step 800, loss 2.33249, accuracy 0.120, mean of rnn weight 0.00275, mean of rnn out 0.03247\n",
      "step 900, loss 2.28118, accuracy 0.150, mean of rnn weight 0.00277, mean of rnn out 0.03504\n",
      "step 1000, loss 2.39003, accuracy 0.100, mean of rnn weight 0.00280, mean of rnn out 0.03296\n",
      "step 1100, loss 2.20053, accuracy 0.140, mean of rnn weight 0.00284, mean of rnn out 0.01565\n",
      "step 1200, loss 2.42153, accuracy 0.120, mean of rnn weight 0.00284, mean of rnn out 0.01546\n",
      "step 1300, loss 2.22281, accuracy 0.210, mean of rnn weight 0.00284, mean of rnn out 0.01465\n",
      "step 1400, loss 2.35044, accuracy 0.110, mean of rnn weight 0.00285, mean of rnn out 0.01542\n",
      "step 2000, loss 2.39010, accuracy 0.120, mean of rnn weight 0.00419, mean of rnn out 0.04801\n",
      "step 3000, loss 2.40588, accuracy 0.120, mean of rnn weight 0.00428, mean of rnn out 0.04671\n",
      "step 4000, loss 2.32964, accuracy 0.110, mean of rnn weight 0.00422, mean of rnn out 0.03118\n",
      "training session done\n"
     ]
    }
   ],
   "source": [
    "batch_size=100\n",
    "init_op=tf.global_variables_initializer()\n",
    "sess=tf.InteractiveSession()\n",
    "sess.run(init_op)\n",
    "\n",
    "variables_names=[v.name for v in tf.trainable_variables()]\n",
    "\n",
    "for step in range(5000):\n",
    "    batch_x,batch_y=mnist.train.next_batch(batch_size)\n",
    "    batch_x=np.reshape(batch_x,(batch_size,n_steps,n_input))\n",
    "    cost_train,accuracy_train,states_train,rnn_out=sess.run([cost,accuracy,states,h[-1]],feed_dict={x:batch_x,y:batch_y})\n",
    "    values=sess.run(variables_names)\n",
    "    rnn_out_mean=np.mean(rnn_out)\n",
    "    \n",
    "    for k,v in zip(variables_names,values):\n",
    "        if k== 'rnn/basic_rnn_cell/weights:0': #k=='RNN/BasicRNNCell/Linear/Matrix:0':\n",
    "            w_rnn_mean=np.mean(v)\n",
    "    if step<1500:\n",
    "        if step % 100 == 0:\n",
    "            print(\"step %d, loss %.5f, accuracy %.3f, mean of rnn weight %.5f, mean of rnn out %.5f\" % (step, cost_train, accuracy_train, w_rnn_mean, rnn_out_mean))\n",
    "    else:\n",
    "        if step%1000 == 0: \n",
    "            print(\"step %d, loss %.5f, accuracy %.3f, mean of rnn weight %.5f, mean of rnn out %.5f\" % (step, cost_train, accuracy_train, w_rnn_mean, rnn_out_mean))\n",
    "            \n",
    "    optimizer.run(feed_dict={x:batch_x,y:batch_y})\n",
    "    \n",
    "print(\"training session done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final loss 2.37063, accuracy 0.09820\n"
     ]
    }
   ],
   "source": [
    "cost_test, accuracy_test = sess.run([cost, accuracy], feed_dict={x: np.reshape(mnist.test.images, [-1, 28, 28]), y: mnist.test.labels})\n",
    "print(\"final loss %.5f, accuracy %.5f\" % (cost_test, accuracy_test) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.         -0.99998403  1.          1.          0.99994344 -0.99999428\n",
      " -1.          1.         -0.99999982  1.          1.          1.\n",
      " -0.99999988 -0.9999637  -1.         -0.99999183  0.99999082 -1.\n",
      "  0.99999988  1.          1.         -0.99999994 -0.9999997  -0.99999988\n",
      " -1.          1.          1.          1.          1.          0.99999928\n",
      "  0.99999642  1.         -1.          1.         -1.          1.         -1.\n",
      "  0.99996853  1.         -1.          1.         -1.          1.         -1.\n",
      " -0.99999928 -0.99999994  0.99999982  1.          0.99999917  1.         -0.9999882\n",
      " -1.          1.          0.99999857  1.         -0.99999994  1.00000012\n",
      " -0.9999975  -1.          1.         -1.         -1.         -1.          1.\n",
      "  0.99999797 -0.99999994 -1.         -1.          1.         -1.         -1.\n",
      "  1.         -0.9999975  -1.         -1.          1.         -0.99994415\n",
      " -0.99999994  1.         -1.          1.          1.          1.         -1.\n",
      " -0.99999541 -1.         -1.          0.992136   -0.99999416  1.          1.\n",
      " -0.9999975   0.99999982  1.         -1.         -1.          0.99997681\n",
      "  0.99999744 -1.         -1.          1.          0.99999988 -1.          1.\n",
      "  0.99999917  1.         -1.          0.99997348  1.         -1.\n",
      "  0.99999988 -1.          1.         -1.          0.99999928  1.          1.\n",
      "  0.99955577 -1.          1.         -0.9998908   0.99999917 -1.         -1.\n",
      " -1.         -1.         -1.          0.99999428]\n"
     ]
    }
   ],
   "source": [
    "print h[-1].eval(feed_dict={x: np.reshape(mnist.test.images, [-1, 28, 28]), y: mnist.test.labels})[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, loss 2.30438, accuracy 0.110, mean of rnn weight 0.00019, mean of rnn out 0.00221\n",
      "step 100, loss 0.47128, accuracy 0.890, mean of rnn weight -0.00620, mean of rnn out -0.01431\n",
      "step 200, loss 0.14613, accuracy 0.960, mean of rnn weight -0.00662, mean of rnn out -0.01598\n",
      "step 300, loss 0.32769, accuracy 0.930, mean of rnn weight -0.00778, mean of rnn out -0.00711\n",
      "step 400, loss 0.26119, accuracy 0.930, mean of rnn weight -0.00861, mean of rnn out 0.00630\n",
      "step 500, loss 0.02981, accuracy 0.990, mean of rnn weight -0.00875, mean of rnn out 0.01064\n",
      "step 600, loss 0.16207, accuracy 0.960, mean of rnn weight -0.00971, mean of rnn out 0.00136\n",
      "step 700, loss 0.06180, accuracy 0.980, mean of rnn weight -0.00948, mean of rnn out -0.00026\n",
      "step 800, loss 0.15753, accuracy 0.960, mean of rnn weight -0.00958, mean of rnn out 0.00031\n",
      "step 900, loss 0.02525, accuracy 1.000, mean of rnn weight -0.00992, mean of rnn out 0.00317\n",
      "step 1000, loss 0.05837, accuracy 0.980, mean of rnn weight -0.01085, mean of rnn out -0.00760\n",
      "step 1100, loss 0.03251, accuracy 0.990, mean of rnn weight -0.01059, mean of rnn out 0.01607\n",
      "step 1200, loss 0.07180, accuracy 0.980, mean of rnn weight -0.01147, mean of rnn out -0.00829\n",
      "step 1300, loss 0.14715, accuracy 0.950, mean of rnn weight -0.01172, mean of rnn out 0.01183\n",
      "step 1400, loss 0.02901, accuracy 1.000, mean of rnn weight -0.01097, mean of rnn out 0.00566\n",
      "step 2000, loss 0.02964, accuracy 0.990, mean of rnn weight -0.01214, mean of rnn out -0.00634\n",
      "step 3000, loss 0.09891, accuracy 0.960, mean of rnn weight -0.01311, mean of rnn out -0.00483\n",
      "step 4000, loss 0.00431, accuracy 1.000, mean of rnn weight -0.01512, mean of rnn out 0.00838\n",
      "training session done\n"
     ]
    }
   ],
   "source": [
    "#LSTM\n",
    "tf.reset_default_graph()\n",
    "\n",
    "x=tf.placeholder(\"float\",[None,n_steps,n_input])\n",
    "y=tf.placeholder(\"float\",[None,n_classes])\n",
    "\n",
    "weights={\n",
    "    \"w_fc\":weight_variable([n_hidden,n_classes],\"w_fc\")\n",
    "}\n",
    "biases={\n",
    "    \"b_fc\":bias_variable([n_classes],\"b_fv\")\n",
    "}\n",
    "x_transpose = tf.transpose(x, [1, 0, 2])\n",
    "x_reshape = tf.reshape(x_transpose, [-1, n_input])\n",
    "x_split = tf.split(x_reshape, n_steps, 0)\n",
    "lstm_cell=tf.contrib.rnn.BasicLSTMCell(n_hidden,forget_bias=1.0)\n",
    "h,states=tf.contrib.rnn.static_rnn(lstm_cell,x_split,dtype=tf.float32)\n",
    "\n",
    "h_fc=tf.matmul(h[-1],weights['w_fc']+biases['b_fc'])\n",
    "y_=h_fc\n",
    "\n",
    "#cost function\n",
    "cost=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=h_fc,labels=y))\n",
    "optimizer=tf.train.AdamOptimizer(0.01).minimize(cost)\n",
    "\n",
    "#accuracy function\n",
    "correct_prediction=tf.equal(tf.argmax(y_,1),tf.argmax(y,1))\n",
    "accuracy=tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n",
    "\n",
    "batch_size=100\n",
    "init_op=tf.global_variables_initializer()\n",
    "sess=tf.InteractiveSession()\n",
    "sess.run(init_op)\n",
    "\n",
    "variables_names=[v.name for v in tf.trainable_variables()]\n",
    "\n",
    "for step in range(5000):\n",
    "    batch_x,batch_y=mnist.train.next_batch(batch_size)\n",
    "    batch_x=np.reshape(batch_x,(batch_size,n_steps,n_input))\n",
    "    cost_train,accuracy_train,states_train,rnn_out=sess.run([cost,accuracy,states,h[-1]],feed_dict={x:batch_x,y:batch_y})\n",
    "    values=sess.run(variables_names)\n",
    "    rnn_out_mean=np.mean(rnn_out)\n",
    "    \n",
    "    for k,v in zip(variables_names,values):\n",
    "        if k== 'rnn/basic_lstm_cell/weights:0':\n",
    "            w_rnn_mean=np.mean(v)\n",
    "    if step<1500:\n",
    "        if step % 100 == 0:\n",
    "            print(\"step %d, loss %.5f, accuracy %.3f, mean of rnn weight %.5f, mean of rnn out %.5f\" % (step, cost_train, accuracy_train, w_rnn_mean, rnn_out_mean))\n",
    "    else:\n",
    "        if step%1000 == 0: \n",
    "            print(\"step %d, loss %.5f, accuracy %.3f, mean of rnn weight %.5f, mean of rnn out %.5f\" % (step, cost_train, accuracy_train, w_rnn_mean, rnn_out_mean))\n",
    "            \n",
    "    optimizer.run(feed_dict={x:batch_x,y:batch_y})\n",
    "    \n",
    "print(\"training session done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final loss 0.06165, accuracy 0.98360\n"
     ]
    }
   ],
   "source": [
    "cost_test, accuracy_test = sess.run([cost, accuracy], feed_dict={x: np.reshape(mnist.test.images, [-1, 28, 28]), y: mnist.test.labels})\n",
    "print(\"final loss %.5f, accuracy %.5f\" % (cost_test, accuracy_test) )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
